
Context-Attentive Embeddings for Improved Sentence RepresentationsDouwe KielaFORMULA , Changhan WangFORMULA  and Kyunghyun ChoFORMULA  FORMULA  Facebook AI Research; FORMULA  New York University{dkiela,changhan,kyunghyuncho}@fb.comWhile one of the first steps in many NLP systems is selecting what embeddings to use, we argue that such a step is better left for neural networks to figure out by themselves. To that end, we introduce a novel, straightforward yet highly effective method for combining multiple types of word embeddings in a single model, leading to state-of-the-art performance within the same model class on a variety of tasks. We subsequently show how the technique can be used to shed new insight into the usage of word embeddings in NLP systems.
Introduction
It is no exaggeration to say that word embeddings have revolutionized NLP. From early distributional semantic models {{cite:4f2359de-dcac-4875-9094-f760eaa37596}}, {{cite:d63ddcd7-668b-484e-a843-bda37f1d300d}}, {{cite:42cae20b-8713-4c85-8eb0-0f60def89dfe}} to deep learning-based word embeddings {{cite:73d056c0-b408-4a37-8af6-3a457eb00745}}, {{cite:7cb967d1-dc92-4a79-b19d-55e7bd024441}}, {{cite:4476290b-f437-4ff1-8ac8-f0659486f82e}}, {{cite:971fcf71-a87c-4478-8182-74d0f3909eab}}, {{cite:52505920-6cd5-4e51-935e-436fd3fa9ac0}}, word-level meaning representations have found applications in a wide variety of core NLP tasks, to the extent that they are now ubiquitous in the field {{cite:faf78a6c-dd7c-47e4-a436-28743b140815}}.
A sprawling literature has emerged about what types of embeddings are most useful for which tasks. For instance, there has been extensive work on understanding what word embeddings learn {{cite:9fe1b57d-4184-4881-b754-2dc7fcb13440}}, evaluating their performance {{cite:d69d9978-aab7-437e-90e2-d44ac8cd33af}}, {{cite:a10488cc-8058-42d0-9fee-3e4fbbe5ec23}}, {{cite:b64d12fc-db35-470e-aa3f-019e1ae0e8cf}}, specializing them for certain tasks {{cite:73be98c5-731c-4d40-b690-67e930c5c1a2}}, {{cite:6033b90e-f04f-4104-bb26-4d607de7268f}}, {{cite:bc66e04e-253a-4b3c-bbea-459976668018}}, {{cite:dfaf0e74-14ec-4f8f-a058-efa0a0aecbc5}}, {{cite:db86b538-1852-497e-9387-38e68d596e29}} and learning sub-word level representations {{cite:d4b21bcf-9307-4391-9236-1fe6290c6bce}}, {{cite:52505920-6cd5-4e51-935e-436fd3fa9ac0}}, {{cite:c0a51f1d-0fbc-45fa-910c-8402c0ef6d55}}.
That said, word embeddings do not come without their problems: they are hard to evaluate, their usefulness for downstream tasks is hard to predict and they do not take into account contextual information when used (or evaluated). In this work, we address some of the weaknesses of word embeddings by making them context-attentive: we learn to represent word meaning as a weighted combination of a multitude of different word embeddings, where the weighting depends on the context.
While one of the first steps in many NLP systems is selecting what embeddings to use, we argue that such a step is better left for neural networks to figure out by themselves, i.e., that we should include multiple different types of word embeddings and let the model pick. We examine the usefulness of this idea in the setting of sentence representations {{cite:9f6b952d-2cb3-40b3-a5cd-a7d6b0a286e2}}, {{cite:2a80d06d-e1e2-4269-bf74-b83edd04fbef}}, {{cite:20bc1889-2ff1-48bc-bc6c-deeb64d19613}}, {{cite:b6e31a8e-1651-4cf5-90d3-ff54e1c3e3a9}}.
The proposed approach turns out to be highly effective, leading to state-of-the-art performance within the same model class on a variety of tasks, opening up new areas for exploration and yielding insights into the usage of word embeddings.
Why Is This a Good Idea?
Our technique brings several important benefits to NLP applications. First, it is embedding-agnostic, meaning that one of the main (and perhaps most important) hyperparameters in NLP pipelines is made obsolete. Second, as we will show, it leads to improved performance on a variety of tasks. Third, and perhaps most importantly, it allows us to overcome common pitfalls with current systems:

Coverage One of the main problems with NLP systems is dealing with out-of-vocabulary words: our method increases lexical coverage by allowing to take the union over different embeddings.

Multi-domain Standard word embeddings are often trained on a single domain, such as Wikipedia or newswire. Our method allows for effectively combining embeddings from different domains, depending on the context.

Multi-modality In many tasks, multi-modal information has proven useful, yet the question of multi-modal fusion remains an open problem. Our method offers a straightforward solution for the question of how to combine information from different modalities.

Evaluation While it is often unclear how to evaluate word embedding performance, our method allows for inspecting the weights that networks assign to different embeddings, providing a direct, task-specific, evaluation method for word embeddings.

Interpretability and Linguistic Analysis Different word embeddings work well on different tasks. This is well-known in the field, but knowing why this happens is less well-understood. Our method sheds light on which embeddings are preferred in which linguistic contexts, for different tasks, and allows us to speculate as to why that is the case.


Outline
In what follows, we introduce context-attentive embeddings and apply the technique in two state-of-the-art sentence encoders, namely BiLSTM-max {{cite:b6e31a8e-1651-4cf5-90d3-ff54e1c3e3a9}} and shortcut-stacked sentence encoders {{cite:0d917a22-c5c3-4945-b677-cce7da0066ad}}. We evaluate on well-known tasks in the field: natural language inference (SNLI and MultiNLI) and sentiment analysis (SST), and show state-of-the-art performance. Lastly, we perform a variety of small experiments to highlight the general usefulness of our technique and how it can lead to new insights.

Context-Attentive Embeddings
Commonly, NLP systems use a single type of word embedding, e.g., word2vec {{cite:4476290b-f437-4ff1-8ac8-f0659486f82e}}, GloVe {{cite:971fcf71-a87c-4478-8182-74d0f3909eab}} or FastText {{cite:52505920-6cd5-4e51-935e-436fd3fa9ac0}}. We propose giving networks access to multiple types of embeddings, and allowing them to select which embeddings they prefer by assigning each embedding type a weight, depending on the context.
Let FORMULA  be a vector representation of the word FORMULA  and let FORMULA  be the set of word embedding types: FORMULA , then, is a word representation of the type FORMULA  (e.g. FORMULA  is a GloVe embedding). Given a context vector FORMULA , we compute a context-dependent attention mask FORMULA  over the set of embeddings using:
FORMULA 
where FORMULA  is concatenation, FORMULA , FORMULA  and FORMULA  is the hyperbolic tangent. We subsequently apply the attention mask on the concatenated vectors to obtain the context-attended embedding:
FORMULA 
The embeddings are centered and L2-normalized in order to ensure an equal contribution of each embedding type if the weights are identical. The input embeddings are kept fixed during training.
We compare our method against using word embeddings of a single type, and the natural baseline of naively concatenating the embeddings without any weightingWe also tried an “inner-attention” mechanism where the attention weights are not conditioned on a context vector but instead calculated using just the input embeddings, and forward-predicting the weights using only the context vector (similar to a language model), but found the current approach to work best and be the most interesting..
Sentence Encoders
We evaluate the usefulness of context-attentive word embeddings in two different state-of-the-art sentence encoders: the BiLSTM-Max model used in InferSent {{cite:b6e31a8e-1651-4cf5-90d3-ff54e1c3e3a9}} and the recently proposed shortcut-stacked sentence encoder {{cite:0d917a22-c5c3-4945-b677-cce7da0066ad}}.
BiLSTM-Max
For a sequence of FORMULA  words, FORMULA  a standard bidirectional LSTM (BiLSTM) computes two sets of FORMULA  hidden states, one for each direction:
FORMULA 
For BiLSTM-Max, the hidden states are subsequently concatenated for each timestep to obtain the final hidden states, after which a max-pooling operation is applied over their components to get the final sentence representation:
FORMULA 
In our case, we instead use the context-attended embedding FORMULA , calculated using FORMULA  as the context for the forward model and FORMULA  for the backward model:
FORMULA 
The hidden states are combined in the same way as the standard model, using max-pooling.

Shortcut-Stacked
The shortcut stacked sentence encoder simply consists of multiple stacked BiLSTMs with shortcut connections followed by a max-pooling operation. The input of the FORMULA -th stacked BiLSTM layer is given as:
FORMULA 
which is recursively applied for the number of layers, where FORMULA . This is followed by a max-pooling operation to obtain the final sentence representation.
In our case, the initial input layer is given as FORMULA  instead, calculated using FORMULA  as the context for the forward model and FORMULA  for the backward model, as above.

Entropy regularization
Even though the embeddings are normalized, the possibility exists that some embedding types get preference due to uneven initialization, in which case embeddings with initial low weights can never recover and remain lowly weighted. We address this through applying orthogonal initialization {{cite:ae127ee0-34cf-4012-8a3d-f099b8c67a91}} on the layers of the attention mechanism, and adding the negative entropy as an additional regularization term to the loss, i.e.,
FORMULA 
where FORMULA  is the task-specific loss and FORMULA  is a regularization coefficient. Minimizing the negative entropy implies that we pay attention to all embeddings, and only pick a specific one when it's beneficial.

Natural Language Inference
Natural language inference, also known as recognizing textual entailment (RTE), is the task of classifying pairs of sentences according to whether they are neutral, entailing or contradictive. Inference about entailment and contradiction is fundamental to understanding natural language, and there are two established datasets to evaluate semantic representations in that setting: SNLI {{cite:3a6244f6-015d-43dd-8caf-8716011c60af}} and the more recent MultiNLI {{cite:6d2a075d-0678-4262-8855-93d189858edd}}.
The SNLI dataset consists of 570k human-generated English sentence pairs, manually labeled for entailment,
contradiction and neutral. The MultiNLI dataset can be seen as an extension of SNLI: it contains 433k sentence pairs, taken from ten different genres (e.g. fiction, government text or spoken telephone conversations), with the same entailment labeling scheme.
Approach
We train sentence encoders with context-attentive embeddings using two well-known and often-used embedding types: FastText {{cite:7ec32acd-9b7d-41c4-b5da-90379b4c7029}}, {{cite:52505920-6cd5-4e51-935e-436fd3fa9ac0}} and GloVe {{cite:971fcf71-a87c-4478-8182-74d0f3909eab}}. Specifically, we make use of the 300-dimensional embeddings trained on a similar WebCrawl corpus, and compare three scenarios: when used individually, when naively concatenated or in the context-attentive setting.
We also compare our approach against other models in the same class—in this case, models that encode sentences individually and do not allow attention across the two sentencesThis is a common distinction, see e.g. the SNLI leaderboard at https://nlp.stanford.edu/projects/snli/.. We include the original works that introduced the respective sentence encoders in the table: InferSent {{cite:b6e31a8e-1651-4cf5-90d3-ff54e1c3e3a9}} for BiLSTM-Max and Shortcut-Stacked Sentence Encoders .
In addition, we include a setting where we combine not two, but four different embedding types, adding FastText wiki-news embeddingsSee https://fasttext.cc/, as well as the BOW2 embeddings from Levy:2014acl trained on Wikipedia.
TABLE 

Results
Table REF  shows the results. We observe that for both sentence encoders, the context-attentive embeddings (marked CAE in the table) outperform naive concatenation as well as using the individual embedding types. The shortcut-stacked sentence encoder performs best, and when used with CAE outperforms all other models. We can increase performance even further by using the four different types instead of just GloVe and FastText (marked Many in the table).
Interestingly, which individual embedding performs best depends on the sentence encoder, with FastText working better for BiLSTM-Max and GloVe working better for shortcut-stacked. The results indicate that CAE leads to a “best of both worlds”, exploiting the respective strengths of the different embedding types. Naive concatenation performed quite well, but we found CAE to work better: it appears to act as a regularizer over the different embeddings, as we found it to have more consistently good behavior across different hyperparameters as well.
Implementation details
The two sentences are represented individually using the sentence encoder. As is standard in the literature, the sentence representations are subsequently combined using FORMULA . We train a two-layer classifier with rectifiers on top of the combined representation. Notice that there is no interaction (e.g., attention) between the representations of FORMULA  and FORMULA  for this class of model.
We tune the sizes of the LSTM encoders and the fully-connected layers in the classifier (512, 1024 or 2048 dimensions), as well as the regularization coefficient, on the validation set. The learning rate is set to FORMULA , dropout to FORMULA , and we optimize using Adam {{cite:21d6fb3a-782a-4497-8baf-c86ce2b1ebee}}. The loss is standard cross-entropy. For MultiNLI, which has no designated validation set, we use the in-domain matched set for validation and report results on the out-of-domain mismatched set.

Sentiment Analysis
To showcase the general applicability of the proposed approach, we also apply it to a case where we have to classify a single sentence, namely, sentiment classification. Sentiment analysis and opinion mining have become important applications for NLP research. We evaluate on the binary SST task {{cite:1c8b58c1-88bf-4d0b-9051-6dc94ad56a60}}, consisting of 70k sentences with a corresponding binary (positive or negative) sentiment label. Table REF  shows that our sentence encoders, if equipped with CAE and GloVe and FastText embeddings, perform at the state of the art as compared to other single-sentence encoders.
TABLE 
FIGURE 
FIGURE 
Implementation details
We tune the same set of hyperparameters on the validation set as in the previous experiment. We found that it was easier to overfit with this dataset, given its size, and got better results optimizing with standard SGD with a learning rate of FORMULA , shrinking (i.e. multiplying) by FORMULA  every time the validation accuracy does not increase.

Discussion & Analysis
Aside from improved performance, an additional benefit of context-attentive embeddings is that they allow inspection of the weights that the network assigns to the respective embeddings. In this section, we perform a variety of smaller experiments in order to highlight the usefulness of the technique for studying linguistic phenomena, determining appropriate training domains and evaluating word embeddings.
TABLE 
Visualizing Contextual Attention
Figure REF  shows the attention weights for GloVe and FastText embeddings, assigned by the best-performing BiLSTM-Max network on the SNLI task. The network has learned to disentangle words that are likely to be close in the same pre-trained embedding spaces but have different meanings (e.g. girl-boy, man-woman and red-blue) by actually using entirely different embeddings for them. We found a similar effect for the sentiment classification models, where opposites are selected from different embedding spaces to facilitate distinguishing between sentiment. See Table REF  for some examples of highest and lowest weighted words for the embedding types.

Linguistic Analysis
TABLE 
We perform a fine-grained analysis of the behavior of context-attentive embeddings on the validation set of SNLI. Table REF  shows a breakdown of the average attention weights per part of speech, for open versus closed class, and the overall weight assignment. The analysis allows us to make several interesting observations: it appears that FastText embeddings are highly preferred for open class words (e.g., nouns, verbs, adjectives and adverbs), while closed class words get more evenly divided attention. Out of the open class words, GloVe does best on verbs.
We can further examine the average attention weights by analyzing them in terms of frequency and concreteness. We use Peter Norvig's English frequency counts from the Google N-grams corpushttp://norvig.com/mayzner.html to divide the words into frequency bins. For concreteness, we use the concreteness ratings from Brysbaert:2014brm. Figure REF  shows the average attention weights per frequency or concreteness bin, ranging from low to high. We observe that there is a general preference for FastText embeddings across all frequency bins. Interestingly, GloVe gets higher attention weights for abstract words, while FastText is strongly preferred for concrete ones.

Multi-Domain Embeddings
TABLE 
The MultiNLI dataset consists of various genres. This allows us to inspect the applicability of source domain data for a specific genre. We train embeddings on three kinds of data: Wikipedia, the Toronto Books Corpus {{cite:21e03499-c1c6-4cc5-aed8-cb609ced2be6}} and the English OpenSubtitleshttp://opus.nlpl.eu/OpenSubtitles.php. We examine the attention weights on the five genres in the in-domain (matched) set, consisting of fiction; transcriptions of spoken telephone conversations; government reports, speeches, letters and press releases; popular culture articles from the Slate Magazine archive; and travel guides.
TABLE 
Table REF  shows the average attention weights for the three embedding types over the five genres. We observe that Toronto Books, which consists of fiction, is very appropriate for the fiction genre, while Wikipedia is highly preferred for the government genre, where formal language is preferred. Slate and fiction make more use of OpenSubtitles. Surprisingly, the spoken telephone genre does not appear to prefer OpenSubtitles, which we might have expected given that they both consist of dialogue.
TABLE 

Evaluating Lexical Specialization
Given the recent interest in the community in specializing, retro-fitting and counter-fitting word embeddings for given tasks, we examine whether the lexical-level benefits of specialization extend to sentence-level downstream tasks. After all, one of the main motivations behind work on lexical entailment is that it allows for better downstream textual entailment. Hence, we take the LEAR embeddings by Vulic:2017arxiv, which currently hold the state of the art on the HyperLex lexical entailment evaluation dataset {{cite:bb170174-6a18-44bc-980f-ea0fd71851d0}}. We compare their best-performing embeddings against the original embeddings that were used for specialization, derived from the BOW2 embeddings of Levy:2014acl. Since LEAR incorporates hierarchical information in the norms of the embeddings, we examine both the normalized and unnormalized case.
Table REF  shows that, unfortunately, lexical specialization does not entail (pun intended) sentence-level improvement. Context-attentive embeddings allow us to inspect the attention weights of each embedding type, and the findings suggest that even though there is a small improvement, the network almost never selects the LEAR embeddings. This experiments illustrates how the proposed approach can be useful for evaluating embeddings, and the question of lexical specialization and how it transfers to downstream tasks is worth further investigation by the community.

Utilizing Different Contexts Windows
It has been found that the size of a context window when learning embeddings has an impact on their performance {{cite:f3b47e1b-947d-4908-9660-dc4cd3b66893}}, {{cite:3f3f4249-6038-4365-ad52-08363cf627ac}}: bigger context windows are likely to lead to more topical embeddings, while smaller contexts are probably more syntactically oriented.
Context-attentive embeddings allow us to examine this question. We train word embeddings on an identical corpus of English Wikipedia using FastText, all with the same hyperparameters except for a different window size, and use them as input for training a BiLSTM-max model on SNLI. Table REF  shows the results on the SNLI dev set, for two BiLSTM-max models trained on that task: one using skip-gram embeddings and one using CBOW embeddings with four different window sizes, including very small ones to relatively big ones: 2, 5, 10 or 20.
We observe differences in the attention over embeddings trained with different window sizesTo ensure a meaningful comparison between embeddings of different window sizes, the models were trained with an entropy regularization coefficient of FORMULA , which aggressively smooths out the attention distribution. We still see clear differences, which would have even greater with a smaller coefficient.. For instance, both models prefer smaller context windows for adverbs and pronouns; and in both cases the window size of 20 is not used as much for adjectives, adverbs and pronouns. In the case of skip-gram, the distribution for nouns and verbs is pretty even, while for CBOW a window size of 20 is again too big. Overall, skip-gram prefers smaller context windows while CBOW's highest-weighted context-window size is 10. This experiment illustrates how context-attentive embeddings can be used to gain insight into the qualities of different types of embeddings.

Related Work
Thanks to their widespread popularity in NLP, a sprawling literature has emerged about learning and applying word embeddings—much too large to fully cover here, so we focus on previous work that combines multiple embeddings for downstream tasks.
Maas:2011acl combine unsupervised embeddings with supervised ones for sentiment classification. Yang:2017iclr and Miyamoto:2016arxiv learn to combine word-level and character-level embeddings. Contextual representations have been used in neural machine translation as well, e.g. for learning contextual word vectors and applying them in other tasks {{cite:23d55c92-74da-451f-9e2d-ce6e10a61c8b}} or for learning context-dependent representations to solve disambiguation problems in machine translation Choi:2016arxiv.
Neural tensor skip-gram models learn to combine word, topic and context embeddings {{cite:d9ea88b0-967b-43d5-a674-4c92e306a9cf}}; context2vec {{cite:dc288193-2126-4280-83ee-ac4cc81a81a2}} learns a more sophisticated context representation separately from target embeddings; and Li:2016kbs learn word representations with distributed word representation with multi-contextual mixed embedding. Recent work in “meta-embeddings”, which by ensembles embedding sets, has been gaining traction Yin:2015arxiv,Coates:2018naacl—here, we show that the idea can be applied in context, and to sentence representations. Peters:2018arxiv recently proposed deep contextualized word representations derived from language models, which led to impressive performance on a variety of tasks.
There has also been work on learning multiple embeddings per word {{cite:35267722-7a6a-4427-bf44-41c49add2975}}, {{cite:d489bfee-8900-48e3-a9fc-9f6efe41c880}}, {{cite:56cc6666-2686-4d5e-8893-cc10121f4307}}, including a lot of work in sense embeddings where the senses of a word have their own individual embeddings {{cite:f9c6bc8c-cce6-45ae-81cd-34ffe11d30f1}}, {{cite:e9ea0259-e9c8-4e7f-b2b3-0a91db90254b}}, as well as on how to apply such sense embeddings in downstream NLP tasks {{cite:1934ec91-7f0c-4242-b162-ba071656094c}}.
The question of combining multiple word embeddings is related to multi-modal and multi-view learning. For instance, combining visual features from convolutional neural networks with word embeddings has been examined {{cite:ea55dedc-f54a-4690-9a17-118dc2a1e1ea}}, {{cite:2140730e-93e1-4165-882a-6a5624e3198c}}, see Baltruvsaitis:2018pami for an overview. There has also been work on unifying multi-view embeddings from different data sources {{cite:deda796c-3220-4360-954e-e948447c8381}}.
The usefulness of different embeddings as initialization has been explored {{cite:af4f1255-f631-4e79-961e-35c1a7570a2f}}, and different architectures and hyperparameters have been extensively examined {{cite:5ef42544-04dd-498a-bc22-95fe246063e8}}. Problems with evaluating word embeddings intrinsically are well known {{cite:b42588d8-ed1d-4191-a284-458401e62d23}}, and various alternatives for evaluating word embeddings in downstream tasks have been proposed {{cite:3b8ce524-bba1-4c73-b396-f64ba847b65c}}, , {{cite:19334c00-5425-42ee-92e6-56abcc70c8a0}}. For more related work with regard to word embeddings and their evaluation, see Bakarov:2017arxiv.
At a higher level, our work draws inspiration from the well-known attention mechanism {{cite:db82e82e-8b80-46e2-a2ff-958407aeeeeb}}, and also relates to self-attention {{cite:9b0c1104-4592-4996-9c49-e49a2f32afce}} and inter-attention {{cite:fc00e9e1-2c5c-42c7-bf3e-c31227cdb21b}}, where the attention mechanism is applied within the same representation as opposed to learning to align multiple sentences.

Conclusion
We argue that the decision of which word embeddings to use in what setting should be left to the neural network. While people usually pick one type of word embeddings for their NLP systems and then stick with it, we find that a context-attentive approach, where embeddings are selected depending on the context, leads to better results. In addition, we showed that the proposed mechanism leads to better interpretability and insightful linguistic analysis. We showed that the network learns to select different embeddings for different data and different tasks.
In future work, we plan to apply this idea to different tasks: it would be interesting to analyze what contexts get picked in multi-modal settings, for example in image-caption retrieval, and to explore what kinds of embeddings are most useful for core NLP tasks, such as tagging, chunking, named entity recognition, and parsing. It would also be interesting to further examine specialization and how it transfers to downstream tasks. In addition, it would be interesting to explore how the attention weights change during training, and if, e.g., annealing the entropy regularization coefficient might help. We limited ourselves to learning representations, but the same idea can be applied to generative settings, such as in language modeling or machine translation.
